{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82214333",
   "metadata": {},
   "source": [
    "# Downloading GPT2 (Generative pre-trained transformer) from Open-AI\n",
    "\n",
    "\n",
    "This notebook contains a begginers guide to the usage of transformers library. It covers:\n",
    "\n",
    "- 0: installation\n",
    "- 1: loading model\n",
    "- 2: basic text generation\n",
    "- 3: accessing next token probabilities\n",
    "- 4: accessing hidden states\n",
    "- 5: acessing model paramenters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b072fc",
   "metadata": {},
   "source": [
    "#### Step 0: Install libraries\n",
    "\n",
    "You need to install transformers and pytorch. Run from bash shell:\n",
    "\n",
    "```{bash}\n",
    "! pip install transformers\n",
    "! pip install torch\n",
    "```\n",
    "\n",
    "#### Step 1: Load GPT-2\n",
    "\n",
    "How can we use GPT-2? Basically, we have two options:\n",
    "\n",
    "- **the \"low level\" option**: use GPT-2 specific API (Application Programming Interface). \n",
    "\n",
    "  - Provides finer control. You specify attention masks, padding tokens, decoding, etc. You’re interacting **directly** with the model and tokenizer objects. \n",
    "  - Better for customization** — e.g., adding constraints, working with batches, doing masked generation, etc.\n",
    "\n",
    "    ```{python}\n",
    "        from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    ```\n",
    "\n",
    "\n",
    "- **the \"high level\" option**: use the  `transformers.pipeline` API. \n",
    "  - This is a \"wrapper\" around the model. You don’t need to manually encode or decode anything. Automatically handles tokenization, decoding and attention under the hood.\n",
    "  - It is faster and easier to use.\n",
    "\n",
    "    ```{python}\n",
    "        from transformers import pipeline, set_seed\n",
    "        set_seed(42)\n",
    "        input_text = [\"Hello\", \"Hello dear!\"]\n",
    "        generator = pipeline('text-generation', model='gpt2', device=-1)\n",
    "        generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
    "    ```\n",
    "\n",
    "Lets look at a first example of text generation, using both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46b307",
   "metadata": {},
   "source": [
    "### 2: Basic Text Generation\n",
    "\n",
    "#### 2.1: GPT-2 API.\n",
    "\n",
    "GPT-2 is provided as an API (Application Programming Interface) inside the Python library Transformers from HuggingFace.\n",
    "This library allows us to load pre-trained models and easily use them for tasks like text generation.\n",
    "\n",
    "From the main page of the Transformers documentation, look for: Transformers/API/Text Models/GPT-2. Alternatively, follow [this link](https://huggingface.co/docs/transformers/en/model_doc/gpt2).\n",
    "\n",
    "##### A first basic example of text generation.\n",
    "\n",
    "- **Import the required classes**\n",
    "\n",
    "We import `transformers.GPT2Tokenizer` and `transformers.GPT2LMHeadModel`. \n",
    "These allow us to tokenize (convert text into tokens the model can process) and load the model.\n",
    "```python\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "```\n",
    "\n",
    "- **Model selection (pt.1)**\n",
    "\n",
    "We load the _smallest_ GPT-2 model and tokenizer by default using:\n",
    "```python\n",
    "model_name = \"gpt2\"  # You can also try \"gpt2-medium\", \"gpt2-large\", or \"gpt2-xl\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "```\n",
    "This loads the pre-trained weights from HuggingFace. The `from_pretrained()` method is inherited from `transformers.PreTrainedModel`, and allows you to load weights, configurations, etc (inherits all its methods). Check documentation!\n",
    "\n",
    "- **About the components**\n",
    "\n",
    "1. _Tokenizer class_: this class takes care of converting text into tokens (integers that correspond to words, subwords, or characters in the model’s vocabulary) and vice versa. If you use the flag return_tensors='pt', the tokenizer returns PyTorch tensors (instead of just a list of token IDs). This is necessary because the model expects tensors as input; \n",
    "\n",
    "Example:\n",
    "```python\n",
    "inputs = tokenizer(\"Once upon a time\", return_tensors='pt')\n",
    "```\n",
    "\n",
    "2. _LMHeadModel class_: this class represents the GPT-2 model with a language modeling head (a linear layer that predicts the probability distribution over the vocabulary for the next token). The model predicts the next token given the previous tokens. In general we can have multiple heads: you can think of the as \"specialists\" trained to focus on a different aspect of language (syntax, long-range dependencies, punctuation, ...); \n",
    "\n",
    "\n",
    "3. _Padding_: when you provide multiple input sequences (e.g., a batch of sentences), they will often have different lengths.\n",
    "Neural networks need inputs of the same size, so padding tokens are added (usually to the right or left of the sequences) to make them the same length.\n",
    "\n",
    "Example:\n",
    "\"Hello\" → [15496, 0, 0]\n",
    "\"Hello world\" → [15496, 995, 0]\n",
    "\n",
    "NOTICE: Here 0 could represent a padding token (GPT-2 doesn't have an official padding token, but you can define one if needed).\n",
    "\n",
    "4. _Attention mask_:\n",
    "The model needs to know which tokens are real content and which are padding.\n",
    "The attention mask is a tensor with:\n",
    "\n",
    "1 → position contains a real token (to attend to)\n",
    "\n",
    "0 → position contains padding (to ignore)\n",
    "\n",
    "The tokenizer generates this mask automatically and it is returned by the tokenizer under the key “attention_mask”.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "inputs = tokenizer([\"Hello\", \"Hello world\"], return_tensors=\"pt\", padding=True)\n",
    "inputs[\"attention_mask\"]\n",
    "```\n",
    "\n",
    "**More about model selection (there are many other possibilities beyond the one shown above)...**\n",
    "\n",
    "- **GPT2Model**: the bare GPT-2 transformer model, the core transformer network. It includes: token embeddings, positional embeddings, stacked transformed blocks (self-attention + feedforward layers + layer norms), final hidden states (the outputs of the transofrmer layers). It does not include: a final linear layer that maps hidden states to vocabulary logits (i.e. no prediction head).\n",
    "It outputs the hidden states(the embedded buffer, X, after all the transformations. Namely, contextualized token representations). It is useful when you want to:  \n",
    "\n",
    "1. Use the transformer’s attention + representation power, but apply your own task-specific head (e.g. classification, regression, custom scoring);\n",
    "\n",
    "2.  analyze or visualize the hidden states or attention weights.\n",
    "Example: \n",
    "```python\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"The quick brown fox\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "hidden_states = outputs.last_hidden_state  # shape: (batch_size, sequence_length, hidden_dim)\n",
    "```\n",
    "Here, \"hidden_states\" contains the vector representations for each token after the transformer layers.\n",
    "\n",
    "- **GPT2LMHeadModel**: is the GPT-2 transformer plus a language modeling head on top. Outputs token probabilities (aka **logits**) so you can do text generation or compute next-token likelihoods.\n",
    "\n",
    "Formula:\n",
    "$$\n",
    "p(t_i) \\propto exp(\\vec{x}_N\\cdot \\vec{x}_{t_i})\n",
    "$$ \n",
    "where $\\vec{x}_N$ is the final hidden state for the last token, and $vec{x}_{t_i}$ is the embedding of candidate token $t_i$ (this is computed $\\forall t_i$).\n",
    "\n",
    "_NOTICE_ There are two types of _heads_ you shoud not get confused about:\n",
    "\n",
    "1. **Attention heads**: individual parallel attention mechanisms inside each transformer layer. \n",
    "\n",
    "For example GPT-2 has $n_{head} = 12$, each block has 12 attention heads. \n",
    "In each head, we compute: \n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt(dk)})V\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- hidden_state: output from last transformer layer\n",
    "- W_lm: matrix mapping hidden dimension → vocab size\n",
    "\n",
    "2. **LM head (language modeling head)**: linear projection (dense layer) that maps hidden states to vocabulary logits. Maps each token’s final hidden vector → probability distribution over vocabulary → generates text.\n",
    "\n",
    "$$ logits = hidden_{state} \\cdot W_{text{lm}}^T + b_{\\text{lm}} $$\n",
    "\n",
    "- **GPT2DoubleHeadsMdoel**: has both the layer that calculates the probabilities and a layer for classification (whatever it is). Useful for multiple-choice tasks (e.g., pick the right ending to a story). \n",
    "\n",
    "Also check `transformers.GenerationMixin.generate()` and its documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63095b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0: \n",
      "Decoded input:  <|endoftext|><|endoftext|>Hello\n",
      "Encoded input:  tensor([50256, 50256, 15496])\n",
      "Attention mask:  tensor([0, 0, 1])\n",
      "Decoded output:  <|endoftext|><|endoftext|>Hello, I am not sure what I would like to hear. What would you like? What would you like me to hear? You're not sure.\n",
      "\n",
      "You know what I would like to know? You're not sure. What\n",
      "_________________________\n",
      "Sequence 1: \n",
      "Decoded input:  Hello dear!\n",
      "Encoded input:  tensor([15496, 13674,     0])\n",
      "Attention mask:  tensor([1, 1, 1])\n",
      "Decoded output:  Hello dear!\n",
      "\n",
      "This is my first post, so I'll start with my thoughts on how to use this tool, how to make it easy for you to use and how it works for you. The first thing I will do is to make\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "# Import tokenizer and model\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Padding in the following is left because GPT-2 is decoder-only and usually generates right-to-left.\n",
    "# Briefly:\n",
    "#   ENCODER -> processes the entire input sequence all at once and produce a meaningful contextual representation for every input token\n",
    "#   DECODER -> generate the output sequence token by token \n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side = \"left\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define the input (different lengths, so padding will be needed)\n",
    "input_text = [\"Hello\", \"Hello dear!\"]\n",
    "\n",
    "# Set the pad token \n",
    "# GPT-2 doesn't have a native pad token (it was trained without one)\n",
    "# So here you're telling it \"when padding, use the EOS (End Of Sequence) as padding token\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize with padding (strings -> token IDs)\n",
    "# Returns a dictionary\n",
    "# {\n",
    "#  'input_ids': tensor of shape (2, max_sequence_length),\n",
    "#  'attention_mask': tensor of 1s (for real tokens) and 0s (for pads)\n",
    "# }\n",
    "# Ex:\n",
    "# input_ids =\n",
    "# [[ <pad> <pad> token1 token2 ],\n",
    "#  [ <pad> token1 token2 token3 ]]\n",
    "# attention_mask =\n",
    "# [[0 0 1 1],\n",
    "#  [0 1 1 1]]\n",
    "padded_sequences = tokenizer(input_text, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the output\n",
    "output = model.generate(padded_sequences[\"input_ids\"], # takes the input tokens (to produce a sequence continuation)\n",
    "                        attention_mask=padded_sequences[\"attention_mask\"], # ensures model ignores padded tokens (so attention = 0 on pad tokens)\n",
    "                        pad_token_id=tokenizer.eos_token_id, # tells it what token id to consider as pad when generating\n",
    "                        max_length=50, # Length of output \n",
    "                        do_sample=True,# If TRUE, tells the model to sample randomly from the top_k most likely tokens instead of always choosing the most likely token - this makes the output more creative\n",
    "                        top_k=5) # Flag is used only if do_sample = TRUE.\n",
    "\n",
    "# Loop through and print\n",
    "# NOTICE: encoding and decoding here have nothing to do with encoder/decoder blocks of the transformer\n",
    "# Encoded input -> token IDs (integers) that correspond to the input string (what the tokenizer outputs when it “encodes” text into model input)\n",
    "# Decoded input -> converting those token IDs back into text (what the tokenizer does when it “decodes” token IDs into readable text)\n",
    "for i in range(output.shape[0]):\n",
    "    print(f\"Sequence {i}: \")\n",
    "    print(\"Decoded input: \", tokenizer.decode(padded_sequences[\"input_ids\"][i]))\n",
    "    print(\"Encoded input: \", padded_sequences[\"input_ids\"][i])\n",
    "    print(\"Attention mask: \", padded_sequences[\"attention_mask\"][i])\n",
    "    print(\"Decoded output: \", tokenizer.decode(output[i],skip_special_tokens=False))\n",
    "    print(\"_________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92fbb04",
   "metadata": {},
   "source": [
    "_NOTICE:_\n",
    "\n",
    "In sequence 0 50256 is GPT-2's eos_token_id, corresponding to token <|endoftext|>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1797d",
   "metadata": {},
   "source": [
    "#### 2.2: `transformers.pipeline` API.\n",
    "\n",
    "Now we run the same example as above, but using the higher level interface provided by the class `pipeline`. Encoding and decoding is under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fed527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{'generated_text': \"Hello, I'm not sure if you're aware of the fact that I'm a member of the American Association of Chiefs of Police. I'm a\"}]\n",
      "Sequence 0: \n",
      "Decoded input:  Hello\n",
      "Decoded Output: Hello, I'm not sure if you're aware of the fact that I'm a member of the American Association of Chiefs of Police. I'm a\n",
      "_________________________\n",
      "\n",
      "[{'generated_text': \"Hello dear! I'm sorry, but I'm not sure what to do. I'm not sure if I should go back to the hospital or not\"}]\n",
      "Sequence 1: \n",
      "Decoded input:  Hello dear!\n",
      "Decoded Output: Hello dear! I'm sorry, but I'm not sure what to do. I'm not sure if I should go back to the hospital or not\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "# Import pipeline: Hugging Face's high-level helper that wraps model + tokenizer + generation logic into one easy interface\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Set a seed (just to get always same output)\n",
    "set_seed(42)\n",
    "\n",
    "# Define the input\n",
    "input_text = [\"Hello\", \"Hello dear!\"]\n",
    "\n",
    "# Do the generation\n",
    "# For each input, the output of pipeline will be a list of lists\n",
    "# Outer list: one entry per input prompt\n",
    "# Inner list: one entry per returned sequence (if num_return_sequences = 1, inner list has one item)\n",
    "# So:\n",
    "# [\n",
    "#  [\n",
    "#    {\"generated_text\": \"original_input_text + model_generated_continuation\"}\n",
    "#  ]\n",
    "# ]\n",
    "generator = pipeline('text-generation', model='gpt2', device=-1) # Use device=0 for GPU, or device=-1 for CPU\n",
    "output = generator(input_text,\n",
    "                    pad_token_id = 50256,\n",
    "                    truncation = True,\n",
    "                    max_length=30,\n",
    "                    temperature=0.1,\n",
    "                    num_return_sequences=1)\n",
    "\n",
    "for idx, field in enumerate(output):\n",
    "    print() # Put a space from previous automatically generated output\n",
    "    print(f\"Sequence {idx}: \")\n",
    "    print(\"Decoded input: \", input_text[idx])\n",
    "    # print(\"Field:\", field)\n",
    "    print(\"Decoded Output:\", field[0][\"generated_text\"]) # Each field corresponds to a single input prompt\n",
    "    print(\"_________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3f843",
   "metadata": {},
   "source": [
    "### 3. How to access token probabilities\n",
    "\n",
    "For this - as well as many other things - you have to use the low level GPT2 API. \n",
    "\n",
    "The **logits** are the scalar products between the last row of the buffer, after transformation, and each of the embedded tokens in the dictionary:\n",
    "\n",
    "$$ \\vec{x}_N \\cdot E^T $$\n",
    " (here $^T$ stands for \"transposed\")\n",
    "\n",
    "_NOTICE_: E is a K(= vocab.size) x D (= dimensionality of embedding) matrix such that the embedded sequence (embedded buffer) is given by $X_{seq} = T_{matrix} \\cdot E$ where instead $T_{matrix}$ is a N(= number of tokens in the buffer) x K matrix (each row corresponding to a one-hot vector encoding of tokens). \n",
    "\n",
    "(In practice however $X_{seq} = E[token\\_ids]$ which is conceptually the same but easier to compute)\n",
    "\n",
    "N.B: logits are not simply $T_{matrix}$ where each entry is a one-hot encoding because the original $X_{seq}$ has been transformed during the process. Basically each logit ($\\in \\mathbb{R}^K$) is a score for each token in the vocabulary. \n",
    "\n",
    "The **probabilities** are given by:\n",
    "\n",
    "$$ \\propto \\exp^{\\left(-\\frac{\\vec{x}_N \\cdot E^T}{T}\\right) }$$\n",
    " \n",
    "in particular:\n",
    "\n",
    "$$  softmax{(\\frac{\\vec{x}_N \\cdot E^T}{T})}$$\n",
    " \n",
    "\n",
    "where this last T at denominator of the exponent is finally the temperature parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22d100",
   "metadata": {},
   "source": [
    "#### First way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bdabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 50257])\n",
      "\n",
      "tensor([[-36.2872, -35.0111, -38.0791,  ..., -40.5161, -41.3758, -34.9191],\n",
      "        [-85.1435, -82.5817, -88.0494,  ..., -88.4072, -90.8886, -84.2703],\n",
      "        [-86.6003, -85.0928, -92.4016,  ..., -98.3911, -91.8806, -89.0551],\n",
      "        ...,\n",
      "        [-86.1226, -85.5085, -86.6623,  ..., -95.5519, -89.5766, -85.6829],\n",
      "        [ -0.4879,   1.0927,  -3.0591,  ..., -11.6097,  -8.8209,  -1.0988],\n",
      "        [-74.8958, -72.4673, -75.6806,  ..., -83.4975, -78.3614, -74.6660]])\n"
     ]
    }
   ],
   "source": [
    "# Loadinf tokenizer and model, setting the token for padding\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side = \"left\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define a new prompt and tokenize it\n",
    "prompt = \"The American flag's colors are red, blue and\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "# Get the outputs\n",
    "import torch \n",
    "with torch.no_grad(): # because during inference you don't need gradients (while in training gradients are needed so PyTorch can compute updates for model weights via backpropagation)\n",
    "                      # this saves memory and computational cost ecause skips storing intermediate results for gradients\n",
    "    outputs = model(**inputs) # passing tokenized input to the model and getting the model's output\n",
    "                              # ** unpacks the dictionary \"inputs\" into keyword arguments (so it's the same as model(input_ids=..., attention_mask=...) )\n",
    "\n",
    "# Get the logits (using adequate attribute of the class generator)\n",
    "logits = outputs.logits  # Shape: [1, seq_len, vocab_size]\n",
    "                            # 1 -> batch size (you passed one input sequence)\n",
    "                            # seq_len -> number of tokens in your prompt: N\n",
    "                            # vocab_size -> number of tokens in GPT-2's vocabulary (50257 for base GPT-2): K\n",
    "\n",
    "# Print logits size\n",
    "print(logits.shape)\n",
    "\n",
    "# Remove the batch dimension (since it is 1) and print [seq_len, vocab_size] raw logits matrix\n",
    "# Each row corresponds to one position in the prompt\n",
    "# Each row is a vector of 50257 values - one for each possible next token at that position\n",
    "print()\n",
    "print(logits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92fe4c",
   "metadata": {},
   "source": [
    "#### Second way:\n",
    "\n",
    "When calling model.generate() set different types of output to True.\n",
    "\n",
    "```python\n",
    "model.generate(\n",
    "    ...,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    output_logits=True,\n",
    "    output_hidden_states=True,\n",
    "    output_attentions=True\n",
    ")\n",
    "```\n",
    "You're asking generate() to:\n",
    "\n",
    "1. Return detailed intermediate outputs (not just the generated sequences)\n",
    "2. Package them in a GenerateDecoderOnlyOutput onject - a structured dictionary-like object. Inside this object we have sequences, scores, logits, hidden_states, attentions...\n",
    "\n",
    "Documentation [here](https://huggingface.co/docs/transformers/v4.51.3/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)\n",
    "\n",
    "_NOTICE:_\n",
    "Scores are equal to logits in case of greedy decoding, but are different in case of more fancy decoding methods like `beam`, or `top_k`.\n",
    "\n",
    "- Greedy decoding -> the model just picks the token with the highest logit at each step: no extra re-weighting or probabilities applied (score for the token is just its logit).\n",
    "\n",
    "- Beam search/Top-k/Sampling -> these methods track multiple hypotheses at once or apply filters. They modify or re-weight the logits during generation, therefore the final score of a token under these strategies is not just the raw logit anymore, rather it reflects additional constraints/adjustments made during decoding. \n",
    "\n",
    "See the doc page [Generation Strategies](https://huggingface.co/docs/transformers/en/generation_strategies#decoding-strategies). See [here](https://discuss.huggingface.co/t/what-is-the-difference-between-logits-and-scores/79796/3) for forum discussion.\n",
    "\n",
    "Example:\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>sequences</summary>\n",
    "\n",
    "* **Always returned**\n",
    "* **Type:** `torch.LongTensor`\n",
    "* **Content:** Final generated token IDs\n",
    "* **Shape:** `(batch_size, sequence_length)`\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>logits</summary>\n",
    "\n",
    "* **Returned when:** `output_logits=True`\n",
    "* **Type:** `tuple(torch.FloatTensor)`\n",
    "* **Content:** Unprocessed prediction scores before softmax\n",
    "* **Shape of each tensor:** `(batch_size, vocab_size)`\n",
    "* **Tuple length:** up to `max_new_tokens`\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>scores</summary>\n",
    "\n",
    "* **Returned when:** `output_scores=True`\n",
    "* **Type:** `tuple(torch.FloatTensor)`\n",
    "* **Content:** The scores used at each generation step for sampling or selection\n",
    "* **Shape of each tensor:** `(batch_size, vocab_size)`\n",
    "* **Tuple length:** up to `max_new_tokens`\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>attentions</summary>\n",
    "\n",
    "* **Returned when:** `output_attentions=True`\n",
    "* **Type:** `tuple(tuple(torch.FloatTensor))`\n",
    "* **Content:** Attention maps\n",
    "* **Shape of each tensor:** `(batch_size, num_heads, generated_length, sequence_length)`\n",
    "* **Structure:**\n",
    "\n",
    "  * Outer tuple: one per generated token\n",
    "  * Inner tuple: one per decoder layer\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>hidden_states</summary>\n",
    "\n",
    "* **Returned when:** `output_hidden_states=True`\n",
    "* **Type:** `tuple(tuple(torch.FloatTensor))`\n",
    "* **Content:** Hidden state activations\n",
    "* **Shape of each tensor:** `(batch_size, generated_length, hidden_size)`\n",
    "* **Structure:**\n",
    "\n",
    "  * Outer tuple: one per generated token\n",
    "  * Inner tuple: one per decoder layer\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de861ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ids: tensor([[8888,  356,  460,  467,  284]]) \n",
      "\n",
      "Outputs class: <class 'transformers.generation.utils.GenerateDecoderOnlyOutput'> \n",
      "\n",
      "Outputs sequences shape: torch.Size([1, 6]) \n",
      "\n",
      "First output word: Today \n",
      "\n",
      "Whole output: Today we can go to the \n",
      "\n",
      "Index: 262, Value: -80.64991760253906, Decoded:  the\n",
      "Index: 257, Value: -81.82210540771484, Decoded:  a\n",
      "Index: 670, Value: -82.49638366699219, Decoded:  work\n",
      "Index: 1175, Value: -82.70538330078125, Decoded:  war\n",
      "Index: 597, Value: -82.73980712890625, Decoded:  any\n",
      "Index: 3993, Value: -82.82905578613281, Decoded:  sleep\n",
      "Index: 674, Value: -83.12396240234375, Decoded:  our\n",
      "Index: 1194, Value: -83.35417175292969, Decoded:  another\n",
      "Index: 3996, Value: -83.4070816040039, Decoded:  bed\n",
      "Index: 477, Value: -83.7303237915039, Decoded:  all\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load pretrained model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inputs =  tokenizer([\"Today we can go to\"], return_tensors=\"pt\") # attributes: input_ids, attention_mask\n",
    "\n",
    "# Print input tokens (corresponding to each word of input)\n",
    "print(\"Input ids:\", inputs.input_ids, \"\\n\")\n",
    "\n",
    "# Generate the output: in \"here\" you're telling to returned a structured object that contains not just the sequences, but also extra info (e.g., scores, hidden states, attentions...)\n",
    "# If you look at the source code, you'll see: \n",
    "# return GenerateDecoderOnlyOutput(...)\n",
    "outputs = model.generate(**inputs, max_new_tokens = 1,\n",
    "                        return_dict_in_generate = True, # here\n",
    "                        output_scores = True,\n",
    "                        temperature = 1.0,\n",
    "                        output_logits = True,\n",
    "                        output_hidden_states = True,\n",
    "                        output_attentions = True,\n",
    "                        pad_token_id = 50256)\n",
    "\n",
    "# Check this by print the class/type of outputs (not a plain tensor or list or dict!!)\n",
    "print(\"Outputs class:\", type(outputs), \"\\n\") \n",
    "\n",
    "# Prints\n",
    "print(\"Outputs sequences shape:\", outputs.sequences.shape, \"\\n\") # tensor of shape (batch_size, generated_sequence_length)\n",
    "print(\"First output word:\", tokenizer.decode(outputs.sequences[0][0], skip_special_tokens =False), \"\\n\") # Grab first token of first sequence and convert the token ID to string repr. \n",
    "print(\"Whole output:\",tokenizer.decode(outputs.sequences[0][:], skip_special_tokens =False), \"\\n\") # Same for full first sequence of output token IDSs\n",
    "\n",
    "# Take logits for the first generated token (for the first sample in the batch)\n",
    "logits = outputs.scores[0][0]\n",
    "# Find the top-10 highest logits (scores) across the vocabulary\n",
    "# Values -> actual logit scores\n",
    "# Indices -> vocab indices (token IDs) corresponding to these scores\n",
    "top_values, top_indices = torch.topk(logits, k=10, largest=True)  # or largest=False for smallest\n",
    "# For each print index, value and decoded word\n",
    "for idx, val in zip(top_indices.tolist(), top_values.tolist()):\n",
    "    print(f\"Index: {idx}, Value: {val}, Decoded: {tokenizer.decode(idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286fc0a1",
   "metadata": {},
   "source": [
    "### 4: Accessing hidden states\n",
    "\n",
    "\n",
    "Hidden states are the token representations in the embedding space $\\mathbb{R}^D$.\n",
    "\n",
    "We can follow the buffer as it exits each of the layers. Hidden-states of the model at the output of each layer plus the optional initial embedding outputs. In this case, $1 + 12 = 13$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806b85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states number: 13 \n",
      "\n",
      "Shape hidden states attribute: (1, 13, 1, 5, 768)\n",
      "So, each hidden state is shaped (batch_size, layers (including embedding), batch_size/placeholder (e.g., extra dim for grouping), seq_length, hidden_size)\n",
      "\n",
      "Layer 0: torch.Size([5, 768])\n",
      "Layer 1: torch.Size([5, 768])\n",
      "Layer 2: torch.Size([5, 768])\n",
      "Layer 3: torch.Size([5, 768])\n",
      "Layer 4: torch.Size([5, 768])\n",
      "Layer 5: torch.Size([5, 768])\n",
      "Layer 6: torch.Size([5, 768])\n",
      "Layer 7: torch.Size([5, 768])\n",
      "Layer 8: torch.Size([5, 768])\n",
      "Layer 9: torch.Size([5, 768])\n",
      "Layer 10: torch.Size([5, 768])\n",
      "Layer 11: torch.Size([5, 768])\n",
      "Layer 12: torch.Size([5, 768])\n"
     ]
    }
   ],
   "source": [
    "# Check number of decoder layers (including embedding layer)\n",
    "print(\"Hidden states number:\", len(outputs.hidden_states[0]), \"\\n\")\n",
    "\n",
    "# Print shape of hidden states\n",
    "print(\"Shape hidden states attribute:\", np.shape(outputs.hidden_states))\n",
    "print(\"So, each hidden state is shaped (batch_size, layers (including embedding), batch_size/placeholder (e.g., extra dim for grouping), seq_length, hidden_size)\\n\")\n",
    "\n",
    "# Get the hidden vector of the first token position at each layer for the first generated token\n",
    "first_token_hidden_states = [layer[0] for layer in outputs.hidden_states[0]]\n",
    "\n",
    "# Print shapes (NOTICE: does not include the 1)\n",
    "for i, h in enumerate(first_token_hidden_states):\n",
    "    print(f\"Layer {i}: {h.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15939d0",
   "metadata": {},
   "source": [
    "###  5. Accessing model parameters\n",
    "\n",
    "Now we retrieve the set of model parameters that are all learned during the training, and kept fixed during inference. These includes:\n",
    "\n",
    "- the embedding map, E\n",
    "- the attention matrices, in each layer and each head $(W_Q, W_K, W_V)$ \n",
    "- the neural net weights, in each layer\n",
    "\n",
    "\n",
    "_NOTICE_: \n",
    "\n",
    "When instanciating the model using \"from_pretrained()\", dropout is deactivated by default by automatically setting the model to evaluation mode -> model.eval().\n",
    "\n",
    "To train or finetune the model, you should first set it back in training mode with model.train(): this reactivates dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e5b39a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check model architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37a96b",
   "metadata": {},
   "source": [
    "_NOTICE:_\n",
    "\n",
    "The model has two main components:\n",
    "- transformer -> core of GPT-2, made of embeddings + layers\n",
    "- lm_head -> final layer that maps hidden states to logits (one score per vocabulary token)\n",
    "\n",
    "Insider transformer we have: \n",
    "- token embeddings (wte) -> Maps token IDs (0..50256) to 768-dimensional vectors\n",
    "- positional embedding (wpe) -> Adds position information for up to 1024 positions\n",
    "- 12 transform layers (ModuleList) -> each of the 12 blocks has two layerNorms, an attention module and an MLP (feed-foreward net). The attention involves: \n",
    "\n",
    "    - c_attn: computes queries, keys, values (3 × 768 = 2304 outputs)\n",
    "    - c_proj: projects back to hidden size\n",
    "    - attn_dropout: dropout in attention (active only in train mode)\n",
    "    - resid_dropout: dropout on residual connections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe2cf79",
   "metadata": {},
   "source": [
    "We can furthermore retreive all learned parameters of the model as an OrderedDict. \n",
    "\n",
    "state_dict: maps parameter names (strings) → tensors (weights or biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3566bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "# Loop to see all weights tensors' names and shape\n",
    "state_dict = model.state_dict()\n",
    "for name, weights in state_dict.items():\n",
    "    print(name, weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49edf75",
   "metadata": {},
   "source": [
    "Some examples of extraction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0017e91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracts the word/token embedding matrix E from the state_dict\n",
    "embedding_matrix = state_dict[\"transformer.wte.weight\"] \n",
    "\n",
    "# Extract the weights of the language modeling head\n",
    "lm_head_matrix = state_dict[\"lm_head.weight\"]\n",
    "\n",
    "# Check whether embedding_matrix and lm_head_matrix contain exactly the same values\n",
    "torch.equal(embedding_matrix, lm_head_matrix) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3d5046",
   "metadata": {},
   "source": [
    "_NOTICE:_\n",
    "\n",
    "In GPT-2, by design weight tying is used: the same matrix is shared for both\n",
    "input token embeddings and output logits linear layer. \n",
    "\n",
    "WHY?\n",
    "\n",
    "Tying input/output embeddings reduces the number of parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894d35c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
