{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2713edd",
   "metadata": {},
   "source": [
    "# ________________________________________________________________________\n",
    "# Getting started with GPT-2\n",
    "# ________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730f6154",
   "metadata": {},
   "source": [
    "## 0: Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e5fca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement transfomers (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for transfomers\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in ./my_venv/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./my_venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./my_venv/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in ./my_venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./my_venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./my_venv/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in ./my_venv/lib/python3.12/site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./my_venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./my_venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./my_venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install transfomers\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3081e",
   "metadata": {},
   "source": [
    "## 1: Using the model\n",
    "\n",
    "How can we use GPT-2? Basically, we have two options:\n",
    "\n",
    "- **the \"low level\" option**: use GPT-2 specific API. \n",
    "\n",
    "  - Provides finer control. You specify attention masks, padding tokens, decoding, etc. You’re interacting **directly** with the model and tokenizer objects. \n",
    "  - Better for customization** — e.g., adding constraints, working with batches, doing masked generation, etc.\n",
    "\n",
    "\n",
    "- **the \"high level\" option**: use the  `transformers.pipeline` API. \n",
    "  - This is a \"wrapper\" around the model. You don’t need to manually encode or decode anything. Automatically handles tokenization, decoding and attention under the hood.\n",
    "  - It is faster and easier to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfb380d",
   "metadata": {},
   "source": [
    "### 1.1: GPT-2 API.\n",
    "\n",
    "GPT-2 is provided as an API (Application Programming Interface) inside the Python library \"Transformers\" from HuggingFace.\n",
    "\n",
    "From the main page of the Transformers documentation, look for: Transformers/API/Text Models/GPT-2. Alternatively, follow [this link](https://huggingface.co/docs/transformers/en/model_doc/gpt2).\n",
    "\n",
    "#### A first basic example of text generation.\n",
    "\n",
    "- Import `transformers.GPT2Tokenizer` and `transformers.GPT2LMHeadModel`. The flag model=\"gpt2\" loads the GPT-2 model and tokenizer, in the base version (i.e. the smallest sized model). If you want you can specify other variants like: \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\".\n",
    "\n",
    "- **GPT2Tokenizer class**: Contains both the encoder and the decoder. The flag return_tensors='pt' tells the tokenizer to return a PyTorch tensor (not just a list), because that's what the model expects.\n",
    "\n",
    "- **LMHeadModel**: The GPT-2 model architecture that generates text (predicts the next token based on previous tokens).\n",
    "\n",
    "- **Padding**: The input text is always tokenized and converted into a tensor, which is a multidimensional rectangular array. If you want, you may provide an imput consisting of several sequences. In general, after tokenization the sequences will have different lengths. This implies the tokenized input cannot be stored in a tensor as it is. Therefore, a padding token is added to the right or to the left of the sequence to make the dimension homogeneous.\n",
    "\n",
    "- **Attention**: The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them. For the GPT2Tokenizer, 1 indicates a value that should be attended to, while 0 indicates a padded value. This attention mask is in the dictionary returned by the tokenizer under the key “attention_mask”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f1c859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miriamzara/LCP_B_GPT2/my_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0: \n",
      "Decoded input:  <|endoftext|><|endoftext|>Hello\n",
      "Encoded input:  tensor([50256, 50256, 15496])\n",
      "Attention mask:  tensor([0, 0, 1])\n",
      "Decoded output:  <|endoftext|><|endoftext|>Hello-O!\n",
      "\n",
      "This week I'll talk to you about the new feature, which is called \"The Last Chance to Win\".\n",
      "\n",
      "You can see the whole post here:\n",
      "\n",
      "The Last Chance to Win: How to\n",
      "_________________________\n",
      "Sequence 1: \n",
      "Decoded input:  Hello dear!\n",
      "Encoded input:  tensor([15496, 13674,     0])\n",
      "Attention mask:  tensor([1, 1, 1])\n",
      "Decoded output:  Hello dear! You've got me in your arms!\n",
      "\n",
      "I've got you in the arms of your own daughter,\n",
      "\n",
      "I've got you in your own son!\n",
      "\n",
      "I've got you in the hands of your own daughter!\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side = \"left\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = [\"Hello\", \"Hello dear!\"]\n",
    "#input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "padded_sequences = tokenizer(input_text, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(padded_sequences[\"input_ids\"], \n",
    "                        attention_mask=padded_sequences[\"attention_mask\"],\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        max_length=50, # Length of output\n",
    "                        do_sample=True,# If TRUE, tells the model to sample randomly from the top_k most likely tokens instead of always choosing the most likely token - this makes the output more creative\n",
    "                        top_k=5) # Flag is used only if do_sample = TRUE.\n",
    "\n",
    "for i in range(output.shape[0]):\n",
    "    print(f\"Sequence {i}: \")\n",
    "    print(\"Decoded input: \", tokenizer.decode(padded_sequences[\"input_ids\"][i]))\n",
    "    print(\"Encoded input: \", padded_sequences[\"input_ids\"][i])\n",
    "    print(\"Attention mask: \", padded_sequences[\"attention_mask\"][i])\n",
    "    print(\"Decoded output: \", tokenizer.decode(output[i],skip_special_tokens=False))\n",
    "    print(\"_________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d572c3",
   "metadata": {},
   "source": [
    "### 1.2: `transformers.pipeline` API.\n",
    "\n",
    "Now we run the same example as above, but using the higher level interface provided by the class `pipeline`. Encoding and decoding is under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105dee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0: \n",
      "Decoded input:  Hello\n",
      "Decoded Output: Hello, I'm not sure if you're aware of the fact that I'm a member of the American Association of Chiefs of Police. I'm a\n",
      "_________________________\n",
      "Sequence 1: \n",
      "Decoded input:  Hello dear!\n",
      "Decoded Output: Hello dear! I'm sorry, but I'm not sure what to do. I'm not sure if I should go back to the hospital or not\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "set_seed(42)\n",
    "input_text = [\"Hello\", \"Hello dear!\"]\n",
    "generator = pipeline('text-generation', model='gpt2', device=-1) # Use device=0 for GPU, or device=-1 for CPU\n",
    "output = generator(input_text,\n",
    "                    pad_token_id = 50256,\n",
    "                    truncation = True,\n",
    "                    max_length=30,\n",
    "                    temperature=0.1,\n",
    "                    num_return_sequences=1)\n",
    "\n",
    "\n",
    "\n",
    "for idx, field in enumerate(output):\n",
    "    print(f\"Sequence {idx}: \")\n",
    "    print(\"Decoded input: \", input_text[idx])\n",
    "    print(\"Decoded Output:\", field[0][\"generated_text\"])\n",
    "    print(\"_________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ca459",
   "metadata": {},
   "source": [
    "# ___________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a9367c",
   "metadata": {},
   "source": [
    "### Accessing the next token probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e21d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4bacce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      " 0: The\n",
      " 1: ĠAmerican\n",
      " 2: Ġflag\n",
      " 3: 's\n",
      " 4: Ġcolors\n",
      " 5: Ġare\n",
      " 6: Ġred\n",
      " 7: ,\n",
      " 8: Ġblue\n",
      " 9: Ġand\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"The American flag's colors are red, blue and\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for idx, tok in enumerate(tokens):\n",
    "    print(f\"{idx:2}: {tok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73397ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 50257])\n",
      "tensor([[-36.2872, -35.0111, -38.0791,  ..., -40.5161, -41.3758, -34.9191],\n",
      "        [-85.1435, -82.5817, -88.0494,  ..., -88.4072, -90.8886, -84.2703],\n",
      "        [-86.6003, -85.0928, -92.4016,  ..., -98.3911, -91.8806, -89.0551],\n",
      "        ...,\n",
      "        [-86.1226, -85.5085, -86.6623,  ..., -95.5519, -89.5766, -85.6829],\n",
      "        [ -0.4879,   1.0927,  -3.0591,  ..., -11.6097,  -8.8209,  -1.0988],\n",
      "        [-74.8958, -72.4673, -75.6806,  ..., -83.4975, -78.3614, -74.6660]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits  # Shape: [1, seq_len, vocab_size]\n",
    "attentions = outputs.attentions  # List of [1, n_heads, seq_len, seq_len] matrice E_t\n",
    "hidden_states = outputs.hidden_states # \n",
    "\n",
    "print(logits.shape)\n",
    "print(logits[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
