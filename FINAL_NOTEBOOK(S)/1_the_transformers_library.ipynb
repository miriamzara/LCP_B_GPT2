{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82214333",
   "metadata": {},
   "source": [
    "# Downloading GPT2 (Generative pre-trained transformer) from Open-AI\n",
    "\n",
    "\n",
    "This notebook contains a begginers guide to the usage of transformers library. It covers:\n",
    "\n",
    "- 0: installation\n",
    "- 1: loading model\n",
    "- 2: basic text generation\n",
    "- 3: accessing next token probabilities\n",
    "- 4: accessing hidden states\n",
    "- 5: acessing model paramenters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b072fc",
   "metadata": {},
   "source": [
    "#### Step 0: Install libraries\n",
    "\n",
    "You need to install transformers and pytorch. Run from bash shell:\n",
    "\n",
    "```{bash}\n",
    "! pip install transformers\n",
    "! pip install torch\n",
    "```\n",
    "\n",
    "#### Step 1: Load GPT-2\n",
    "\n",
    "How can we use GPT-2? Basically, we have two options:\n",
    "\n",
    "- **the \"low level\" option**: use GPT-2 specific API. \n",
    "\n",
    "  - Provides finer control. You specify attention masks, padding tokens, decoding, etc. You’re interacting **directly** with the model and tokenizer objects. \n",
    "  - Better for customization** — e.g., adding constraints, working with batches, doing masked generation, etc.\n",
    "\n",
    "    ```{python}\n",
    "        from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "        model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "    ```\n",
    "\n",
    "\n",
    "- **the \"high level\" option**: use the  `transformers.pipeline` API. \n",
    "  - This is a \"wrapper\" around the model. You don’t need to manually encode or decode anything. Automatically handles tokenization, decoding and attention under the hood.\n",
    "  - It is faster and easier to use.\n",
    "\n",
    "    ```{python}\n",
    "        from transformers import pipeline, set_seed\n",
    "        set_seed(42)\n",
    "        input_text = [\"Hello\", \"Hello dear!\"]\n",
    "        generator = pipeline('text-generation', model='gpt2', device=-1)\n",
    "        generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
    "    ```\n",
    "\n",
    "Lets look at a first example of text generation, using both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46b307",
   "metadata": {},
   "source": [
    "### 2: Basic Text Generation\n",
    "\n",
    "#### 2.1: GPT-2 API.\n",
    "\n",
    "GPT-2 is provided as an API (Application Programming Interface) inside the Python library \"Transformers\" from HuggingFace.\n",
    "\n",
    "From the main page of the Transformers documentation, look for: Transformers/API/Text Models/GPT-2. Alternatively, follow [this link](https://huggingface.co/docs/transformers/en/model_doc/gpt2).\n",
    "\n",
    "##### A first basic example of text generation.\n",
    "\n",
    "- Import `transformers.GPT2Tokenizer` and `transformers.GPT2LMHeadModel`. The flag model=\"gpt2\" loads the GPT-2 model and tokenizer, in the base version (i.e. the smallest sized model). If you want you can specify other variants like: \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\".\n",
    "\n",
    "- **GPT2Tokenizer class**: Contains both the encoder and the decoder. The flag return_tensors='pt' tells the tokenizer to return a PyTorch tensor (not just a list), because that's what the model expects.\n",
    "\n",
    "- **LMHeadModel**: The GPT-2 model architecture that generates text (predicts the next token based on previous tokens).\n",
    "\n",
    "- **Padding**: The input text is always tokenized and converted into a tensor, which is a multidimensional rectangular array. If you want, you may provide an imput consisting of several sequences. In general, after tokenization the sequences will have different lengths. This implies the tokenized input cannot be stored in a tensor as it is. Therefore, a padding token is added to the right or to the left of the sequence to make the dimension homogeneous.\n",
    "\n",
    "- **Attention**: The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them. For the GPT2Tokenizer, 1 indicates a value that should be attended to, while 0 indicates a padded value. This attention mask is in the dictionary returned by the tokenizer under the key “attention_mask”.\n",
    "\n",
    "\n",
    "\n",
    "**NOTES** Which model should we choose?\n",
    "\n",
    "- GPT2Model: the base transformer, outputs the hidden states(the embedded buffer, X, after all the transformations)\n",
    "- GPT2LMHeadModel: the base transformer, plus the layer which calculates the probabilities (aka the token logits) $p(t_i) \\propto exp(\\vec{x}_N\\cdot \\vec{x}_{t_i})$ for each token $t_i$ in the vocabulary.\n",
    "- GPT2DoubleHeadsModel: has both the layer that calculates the probabilities and a layer for classification (whatever it is). Used for multiple-choice Q&A.\n",
    "\n",
    "\n",
    "We need to use `GPT2LMHeadModel.from_pretrained()`. This is an instance of the class transformers.PreTrainedModel and inherits all its methods. Check its the documentation.\n",
    "\n",
    "\n",
    "Also check `transformers.GenerationMixin.generate()` and its documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63095b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0: \n",
      "Decoded input:  <|endoftext|><|endoftext|>Hello\n",
      "Encoded input:  tensor([50256, 50256, 15496])\n",
      "Attention mask:  tensor([0, 0, 1])\n",
      "Decoded output:  <|endoftext|><|endoftext|>Hello\n",
      "\n",
      "Hi everyone!\n",
      "\n",
      "The last few months have been a whirlwind.\n",
      "\n",
      "I'm so happy and thankful to everyone for taking the time to read through this thread.\n",
      "\n",
      "We've had a great run at this game\n",
      "_________________________\n",
      "Sequence 1: \n",
      "Decoded input:  Hello dear!\n",
      "Encoded input:  tensor([15496, 13674,     0])\n",
      "Attention mask:  tensor([1, 1, 1])\n",
      "Decoded output:  Hello dear! You are my daughter and your daughter's sister. I will be happy to see you soon, but I want your daughter to see you soon. I will be very pleased to hear of your success. Please, please let us meet.\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side = \"left\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = [\"Hello\", \"Hello dear!\"]\n",
    "#input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "padded_sequences = tokenizer(input_text, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(padded_sequences[\"input_ids\"], \n",
    "                        attention_mask=padded_sequences[\"attention_mask\"],\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        max_length=50, # Length of output\n",
    "                        do_sample=True,# If TRUE, tells the model to sample randomly from the top_k most likely tokens instead of always choosing the most likely token - this makes the output more creative\n",
    "                        top_k=5) # Flag is used only if do_sample = TRUE.\n",
    "\n",
    "for i in range(output.shape[0]):\n",
    "    print(f\"Sequence {i}: \")\n",
    "    print(\"Decoded input: \", tokenizer.decode(padded_sequences[\"input_ids\"][i]))\n",
    "    print(\"Encoded input: \", padded_sequences[\"input_ids\"][i])\n",
    "    print(\"Attention mask: \", padded_sequences[\"attention_mask\"][i])\n",
    "    print(\"Decoded output: \", tokenizer.decode(output[i],skip_special_tokens=False))\n",
    "    print(\"_________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f1797d",
   "metadata": {},
   "source": [
    "#### 2.2: `transformers.pipeline` API.\n",
    "\n",
    "Now we run the same example as above, but using the higher level interface provided by the class `pipeline`. Encoding and decoding is under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76fed527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0: \n",
      "Decoded input:  Hello\n",
      "Decoded Output: Hello, I'm not sure if you're aware of the fact that I'm a member of the American Association of Chiefs of Police. I'm a\n",
      "_________________________\n",
      "Sequence 1: \n",
      "Decoded input:  Hello dear!\n",
      "Decoded Output: Hello dear! I'm sorry, but I'm not sure what to do. I'm not sure if I should go back to the hospital or not\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "set_seed(42)\n",
    "input_text = [\"Hello\", \"Hello dear!\"]\n",
    "generator = pipeline('text-generation', model='gpt2', device=-1) # Use device=0 for GPU, or device=-1 for CPU\n",
    "output = generator(input_text,\n",
    "                    pad_token_id = 50256,\n",
    "                    truncation = True,\n",
    "                    max_length=30,\n",
    "                    temperature=0.1,\n",
    "                    num_return_sequences=1)\n",
    "\n",
    "\n",
    "\n",
    "for idx, field in enumerate(output):\n",
    "    print(f\"Sequence {idx}: \")\n",
    "    print(\"Decoded input: \", input_text[idx])\n",
    "    print(\"Decoded Output:\", field[0][\"generated_text\"])\n",
    "    print(\"_________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3f843",
   "metadata": {},
   "source": [
    "### 3. How to access token probabilities\n",
    "\n",
    "For this - as well as many other things - you have to use the low level GPT2 API. \n",
    "\n",
    "The **logits** are the scalar products between the last row of the buffer, after transformation, and each of the embedded tokens in the dictionary:\n",
    "\n",
    "$$ \\vec{x}_N \\cdot E^T $$\n",
    "\n",
    "The **probabilities** are given by:\n",
    "\n",
    "$$ \\exp^{\\left(-\\frac{\\vec{x}_N \\cdot E^T}{T}\\right) }$$\n",
    "\n",
    "where T is the temperature parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22d100",
   "metadata": {},
   "source": [
    "### First way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b72bdabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 50257])\n",
      "tensor([[-36.2872, -35.0111, -38.0791,  ..., -40.5161, -41.3758, -34.9191],\n",
      "        [-85.1435, -82.5817, -88.0494,  ..., -88.4072, -90.8886, -84.2703],\n",
      "        [-86.6003, -85.0928, -92.4016,  ..., -98.3911, -91.8806, -89.0551],\n",
      "        ...,\n",
      "        [-86.1226, -85.5085, -86.6623,  ..., -95.5519, -89.5766, -85.6829],\n",
      "        [ -0.4879,   1.0927,  -3.0591,  ..., -11.6097,  -8.8209,  -1.0988],\n",
      "        [-74.8958, -72.4673, -75.6806,  ..., -83.4975, -78.3614, -74.6660]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side = \"left\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "padded_sequences = tokenizer(input_text, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "prompt = \"The American flag's colors are red, blue and\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "\n",
    "\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits  # Shape: [1, seq_len, vocab_size]\n",
    "print(logits.shape)\n",
    "print(logits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92fe4c",
   "metadata": {},
   "source": [
    "#### Second way:\n",
    "\n",
    "When calling model.generate(), set output_scores oe output_logits to True.\n",
    "\n",
    "\n",
    "Documentation [here](https://huggingface.co/docs/transformers/v4.51.3/en/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput)\n",
    "\n",
    "**Notes**\n",
    "Scores are equal to logits in case of greedy decoding, but are different in case of more fancy decoding methods like `beam`, or `top_k`. See the \n",
    "doc page [Generation Strategies](https://huggingface.co/docs/transformers/en/generation_strategies#decoding-strategies). See [here](https://discuss.huggingface.co/t/what-is-the-difference-between-logits-and-scores/79796/3) for forum discussion.\n",
    "\n",
    "\n",
    "\n",
    "logits (tuple(torch.FloatTensor) optional, returned when output_logits=True) — Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax) at each generation step. Tuple of torch.FloatTensor with up to max_new_tokens elements (one element for each generated token), with each tensor of shape (batch_size, config.vocab_size).\n",
    "\n",
    "attentions (tuple(tuple(torch.FloatTensor)), optional, returned when output_attentions=True) — Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of torch.FloatTensor of shape (batch_size, num_heads, generated_length, sequence_length).\n",
    "\n",
    "hidden_states (tuple(tuple(torch.FloatTensor)), optional, returned when output_hidden_states=True) — Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of torch.FloatTensor of shape (batch_size, generated_length, hidden_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de861ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8888,  356,  460,  467,  284]])\n",
      "torch.Size([1, 6])\n",
      "Today\n",
      "Today we can go to the\n",
      "Index: 262, Value: -80.64991760253906, Decoded:  the\n",
      "Index: 257, Value: -81.82210540771484, Decoded:  a\n",
      "Index: 670, Value: -82.49638366699219, Decoded:  work\n",
      "Index: 1175, Value: -82.70538330078125, Decoded:  war\n",
      "Index: 597, Value: -82.73980712890625, Decoded:  any\n",
      "Index: 3993, Value: -82.82905578613281, Decoded:  sleep\n",
      "Index: 674, Value: -83.12396240234375, Decoded:  our\n",
      "Index: 1194, Value: -83.35417175292969, Decoded:  another\n",
      "Index: 3996, Value: -83.4070816040039, Decoded:  bed\n",
      "Index: 477, Value: -83.7303237915039, Decoded:  all\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load pretrained model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inputs =  tokenizer([\"Today we can go to\"], return_tensors=\"pt\") # atributes: input_ids, attention_mask\n",
    "print(inputs.input_ids)\n",
    "outputs = model.generate(**inputs, max_new_tokens = 1,\n",
    "                        return_dict_in_generate = True,\n",
    "                        output_scores = True,\n",
    "                        temperature = 1.0,\n",
    "                        output_logits = True,\n",
    "                        output_hidden_states = True,\n",
    "                        output_attentions = True,\n",
    "                        pad_token_id = 50256)\n",
    "\n",
    "type(outputs) # transformers.generation.utils.GenerateDecoderOnlyOutput\n",
    "\n",
    "\n",
    "print(outputs.sequences.shape)\n",
    "print(tokenizer.decode(outputs.sequences[0][0], skip_special_tokens =False))\n",
    "print(tokenizer.decode(outputs.sequences[0][:], skip_special_tokens =False))\n",
    "\n",
    "\n",
    "logits = outputs.scores[0][0]\n",
    "top_values, top_indices = torch.topk(logits, k=10, largest=True)  # or largest=False for smallest\n",
    "for idx, val in zip(top_indices.tolist(), top_values.tolist()):\n",
    "    print(f\"Index: {idx}, Value: {val}, Decoded: {tokenizer.decode(idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286fc0a1",
   "metadata": {},
   "source": [
    "### 4: Accessing hidden states\n",
    "\n",
    "\n",
    "Hidden states are the token representations in the embedding space $\\mathbb{R}^D$.\n",
    "\n",
    "We can follow the buffer as it exits each of the layers. Hidden-states of the model at the output of each layer plus the optional initial embedding outputs. In this case, $1 + 12 = 13$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22d9fe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([5, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3067e-02, -2.9040e-01,  1.1197e-01, -3.2291e-03,  2.9128e-03,\n",
       "        -2.1564e-01, -1.8018e-01, -1.8713e-01, -8.6993e-02, -3.5113e-01,\n",
       "        -6.8742e-02, -4.7279e-02,  1.2248e-01,  3.9539e-02,  8.5879e-02,\n",
       "        -1.3172e-01,  1.1782e-01, -6.5752e-02,  7.8041e-02, -1.0637e-01,\n",
       "        -3.3029e-02, -6.2341e-02, -8.8152e-02, -4.7329e-02,  1.4917e-01,\n",
       "        -7.3244e-02, -6.4885e-02,  2.1092e-01,  3.2153e-02,  1.3472e-01,\n",
       "        -2.0389e-02, -3.5955e-01,  5.8926e-02,  3.9860e-03,  9.3552e-02,\n",
       "        -7.5463e-02, -1.0719e+00,  1.0046e-01, -4.4027e-02,  9.4923e-02,\n",
       "         6.0284e-03, -5.4183e-02,  6.3331e-02, -1.3404e-01,  9.1742e-02,\n",
       "        -8.9475e-02,  3.1307e-02, -9.8643e-02, -3.6463e-02,  2.7996e-01,\n",
       "         7.4173e-02, -2.9777e-02,  6.6384e-01, -8.4596e-02, -5.1499e-02,\n",
       "         3.8400e+00, -4.7607e-03,  5.3434e-02, -3.4203e-02, -1.4839e-01,\n",
       "         1.1489e-01, -2.1606e-01, -3.7943e-02, -3.5273e-02,  2.3708e-01,\n",
       "         3.3337e-02, -2.6543e-02, -5.4957e-01,  1.8974e-01, -2.2574e-02,\n",
       "        -1.5232e-02,  2.3822e-01,  1.0959e-02,  4.3647e-02,  9.2457e-02,\n",
       "         3.3140e-02, -5.3460e-02,  8.0384e-02, -8.2433e-03,  3.3274e-02,\n",
       "         1.3650e-01,  1.5159e-01, -6.3297e-02,  2.2260e-02,  1.6161e-01,\n",
       "        -4.9739e-02, -1.1084e+00,  1.9613e-01,  1.1491e-01,  1.1559e-01,\n",
       "         2.9778e-02,  1.0045e-01, -1.9980e-01, -1.5542e-02, -5.1501e-02,\n",
       "        -2.4570e-03, -1.2786e-01,  4.2697e-02, -4.5028e-02, -8.0230e-02,\n",
       "        -1.1739e-02, -1.4583e-01, -1.9842e-01, -2.4590e-01, -4.0496e-02,\n",
       "        -1.5544e-01, -4.2272e-02,  2.9037e-01,  3.7818e-01, -1.5138e-01,\n",
       "        -1.2381e-01, -3.1114e-01, -5.3876e-02,  1.1651e-01,  9.3799e-02,\n",
       "         1.3491e-01, -7.2323e-01,  7.9851e-02,  2.1203e-01,  9.5577e-02,\n",
       "        -2.6045e-02, -2.5658e-01,  4.3526e-02, -2.6286e-01, -1.2754e-01,\n",
       "         2.2683e-02, -7.0446e-02, -5.5626e-02,  4.4800e-02,  1.3283e-01,\n",
       "        -7.4203e-02,  5.7470e-02,  3.2576e-02, -3.8778e-02,  6.4098e-02,\n",
       "        -1.9322e-04,  7.0575e-02, -8.7565e-02, -5.1236e-02, -4.9379e-01,\n",
       "        -1.9849e-01,  4.8773e-02,  1.1582e-01, -8.3781e-02,  2.3143e-01,\n",
       "        -1.7989e-01, -7.3110e-04,  2.2078e-01,  1.6037e-01,  1.0763e-01,\n",
       "         5.5104e-02, -8.5823e-02,  1.2832e-01, -1.9827e-01,  1.7414e-01,\n",
       "        -1.1120e-01,  1.6456e-01, -1.7798e-01, -2.7492e-01, -7.4171e-02,\n",
       "        -2.0338e-01,  1.8436e-01, -1.2121e-01, -6.4832e-02, -4.0458e-02,\n",
       "        -1.1611e-01, -8.4416e-02,  2.7805e-01,  1.8935e-02,  3.4222e-01,\n",
       "         4.1511e-01, -8.2525e-02, -4.6729e-03,  1.3168e-01,  2.1360e-01,\n",
       "        -5.4402e-02, -5.7724e-01,  3.5954e-02,  5.7753e-02,  1.7314e-01,\n",
       "         1.0841e-01,  8.3070e-02,  1.3223e-01,  1.2143e-01, -6.8351e-02,\n",
       "         2.2307e-01,  1.4442e-02,  1.7624e-01,  1.9657e-01, -2.3908e-01,\n",
       "        -1.5385e-01, -1.2672e-02, -3.0672e-03,  2.7832e-01, -1.2797e-01,\n",
       "         1.0725e-01,  1.3672e-01,  1.5317e-01, -2.8857e-02,  2.6240e-01,\n",
       "        -1.1471e-03, -1.5288e-01,  3.4566e-02, -1.1359e-02, -1.3145e-01,\n",
       "        -2.3199e-01,  6.8369e-02, -1.4038e-01, -7.0262e-02, -1.3788e-01,\n",
       "        -4.8451e-02,  5.2140e-02, -7.6289e-02, -4.2740e-02, -4.6685e-02,\n",
       "        -1.0067e-01,  3.1730e-02, -1.3450e-01,  7.1088e-02, -1.3024e-01,\n",
       "        -4.3955e-02, -5.7671e-02, -7.8363e-02, -2.9176e-01, -4.5774e-02,\n",
       "        -3.0082e-02, -6.9496e-02,  1.8215e-01,  1.3717e-01,  1.2691e-01,\n",
       "         1.5116e-01,  5.4914e-02, -3.5236e-01, -1.5176e-01, -2.1661e-01,\n",
       "        -1.6586e-01, -3.4494e-01, -3.2441e-03, -2.4895e-02,  8.2749e-02,\n",
       "        -4.9893e-02,  5.0131e-02,  5.6073e-02,  2.2921e-02, -1.9791e-01,\n",
       "         1.4839e-01,  5.0064e-02, -4.6382e-02,  2.7276e-01,  9.8350e-02,\n",
       "         5.7337e-02, -4.0537e-02,  6.1337e-02,  2.8603e-01,  2.8792e-02,\n",
       "        -6.6694e-02,  6.0709e-02,  1.7711e-02,  1.0556e-01,  7.8265e-02,\n",
       "        -1.4689e-01, -4.3485e-02,  8.0235e-02,  2.5910e-01,  1.4038e-01,\n",
       "        -1.9949e-01,  2.0361e-01,  2.7698e-02,  5.1709e-02, -7.7765e-02,\n",
       "         2.3228e+00,  6.8261e-01,  2.0530e-01, -1.4510e-01, -7.2015e-02,\n",
       "        -5.3947e-02, -4.0885e-01,  7.2925e-03,  3.8649e-02,  8.2781e-02,\n",
       "         1.2204e-01, -1.7927e-01, -2.4441e-01,  7.9010e-02, -1.7663e-01,\n",
       "        -1.7670e-01, -1.3522e-01, -1.4565e-01, -5.0777e-01,  7.4921e-01,\n",
       "         3.6932e-02,  1.4844e-01,  8.3764e-03, -5.2613e-02,  1.7514e-01,\n",
       "         1.1350e-01,  3.6479e-01,  1.3264e-01,  1.9577e-02,  1.8739e-02,\n",
       "         2.9113e+00,  1.0322e-01,  9.7360e-06, -3.3545e-02,  1.4986e-02,\n",
       "         2.5972e-01,  7.6549e-02,  9.1864e-02, -1.0963e-02, -1.5486e-02,\n",
       "        -4.5332e-02,  3.4743e-01, -7.6941e-02, -6.7447e-02,  1.5825e-01,\n",
       "        -1.8154e-02, -5.3904e-03, -8.9722e-03,  1.9684e-01, -8.2055e-02,\n",
       "         3.7994e-01,  2.7682e-02, -1.1567e-01, -3.1825e-01,  2.4941e-02,\n",
       "        -5.7901e-02, -6.0254e-01,  2.0661e-01, -1.5451e-01, -3.9815e-02,\n",
       "         3.1348e-01,  5.5284e-02,  1.2130e-01, -6.3270e-02, -7.0257e-02,\n",
       "        -2.7799e-01,  1.2793e-01,  8.5841e-02, -1.6527e-01,  6.8900e-02,\n",
       "         1.7850e-01,  9.1013e-02, -5.7761e-02, -8.2431e-02, -4.8623e-02,\n",
       "        -1.2189e-01,  4.1207e-02,  1.0398e-02,  9.3997e-02,  2.3972e-01,\n",
       "         2.5231e-01, -3.6174e-01,  1.5928e-02, -1.2788e-01, -1.1733e-01,\n",
       "        -5.1687e-02, -2.2409e+00, -1.8934e-01, -5.1106e-02,  1.0877e-01,\n",
       "         1.1758e-01, -9.0829e-01, -2.2911e-01,  8.7095e-02,  1.3290e-01,\n",
       "        -3.8778e-01, -8.9119e-02,  8.0485e-02, -6.7809e-02, -2.1751e-01,\n",
       "         1.3892e-01, -1.3955e-01,  3.7647e-02,  2.0459e-01, -3.1750e-01,\n",
       "        -8.5849e-02, -2.1196e-02, -1.1332e+00, -3.4345e-01,  7.7092e-02,\n",
       "         1.3277e-01, -3.9675e-02,  9.8059e-02, -1.7925e-01,  9.6114e-02,\n",
       "         7.6242e-02, -4.1878e-02,  1.3869e-01, -2.8840e-01,  2.3867e-01,\n",
       "         3.3804e-02,  2.9276e-02, -7.3935e-02, -5.2584e-02, -1.0961e-01,\n",
       "         1.5337e-01, -9.6543e-03,  5.7942e-02, -5.8994e-02, -1.5140e-01,\n",
       "        -4.6314e-02, -2.0632e-01, -4.6490e-01,  4.0304e-02,  6.4062e-02,\n",
       "        -2.0813e-02,  7.0590e-02, -7.2051e-02, -5.1452e-01,  1.4100e-01,\n",
       "        -1.1851e-01, -3.7016e-02,  9.7081e-02, -3.8700e-02,  6.6000e-02,\n",
       "         2.5124e-02,  1.5013e-01,  6.3978e-02,  6.7246e-02, -2.2893e-01,\n",
       "        -2.5707e-01, -1.8826e-01,  4.8039e-02,  2.4506e-01,  2.0452e-02,\n",
       "        -7.5577e-02, -2.3242e-02, -9.4483e-02,  1.3625e-01,  2.1714e-03,\n",
       "        -3.4670e-01,  5.3059e-02,  6.6623e-02,  1.9529e-01, -1.3778e-01,\n",
       "        -1.5787e-01,  5.7232e-02, -8.3218e-02,  2.1909e-02,  3.2559e-01,\n",
       "        -1.1941e-01, -1.8646e-01,  1.3235e-02, -5.1119e-03,  2.6944e-01,\n",
       "        -1.9504e-01,  2.3907e-01, -1.6987e-01,  6.9685e-02, -5.7586e-02,\n",
       "        -1.9520e-01,  2.1466e-01,  1.1000e-01,  5.5133e-01,  9.5714e-03,\n",
       "        -4.6383e-01,  1.5111e-01,  1.8719e-01, -1.6841e-01,  3.2793e-01,\n",
       "        -1.5141e-01,  2.3538e-02, -1.6740e-02, -2.6573e-02, -2.2479e-02,\n",
       "         9.0270e-02, -6.5544e-02, -1.1021e-01, -5.9926e-02,  5.0834e-02,\n",
       "        -6.7674e-02, -2.3793e-01,  1.8740e-01, -1.0172e-01, -3.4240e-02,\n",
       "        -9.3748e-02,  6.0375e-03,  4.7877e-02,  7.1256e-02, -6.0827e-01,\n",
       "         1.4340e-01, -2.1987e-01, -2.0647e-01,  1.1971e-01, -2.3239e-02,\n",
       "        -1.5496e-01,  6.4268e-02,  1.4744e-01,  4.5549e-02,  1.0308e-01,\n",
       "         8.1619e-02,  1.2370e-01, -2.2466e-01, -6.7190e-02,  8.0993e-02,\n",
       "        -9.1310e-02, -3.9987e-01,  3.2425e-01,  5.3394e-02, -9.5313e-02,\n",
       "         6.1822e-02,  1.8000e-01, -1.3558e-01, -1.3628e+00, -8.2689e-02,\n",
       "        -6.8081e-02,  1.8409e-01, -1.9867e-01, -1.0541e-01, -1.6688e-02,\n",
       "        -5.8210e-02,  5.1436e-01, -3.4241e-02, -9.2057e-03, -5.4025e-02,\n",
       "         8.9756e-02, -2.8502e-01, -1.5593e-01,  7.7636e-02, -4.6946e-02,\n",
       "        -8.3758e-02,  3.4105e-02, -2.7297e-02,  9.5536e-02,  1.7935e-02,\n",
       "        -2.7192e-01,  7.6883e-02,  1.0351e-02, -8.4021e-01,  4.3926e-02,\n",
       "        -2.6076e-01,  1.4370e-01, -4.4635e-02,  2.1801e-01, -2.0435e-01,\n",
       "         7.2224e-02, -7.7271e-02, -1.0122e-01, -1.1866e-02, -7.8697e-04,\n",
       "         1.5468e+00, -3.1247e-01,  3.6208e-02,  2.5189e-01,  1.3516e-02,\n",
       "         1.9088e-01, -6.6776e-02, -1.4094e-01,  6.2540e-02,  2.5902e-01,\n",
       "        -8.8360e-02, -9.5592e-01, -9.6683e-02, -5.6508e-02,  4.3640e-01,\n",
       "         5.0378e-02,  1.1581e-01, -4.7618e-02, -1.3664e-01, -1.6678e-02,\n",
       "        -4.8778e-02,  1.6148e-02, -1.8517e-01,  8.5593e-02,  2.0012e-01,\n",
       "         2.5356e-02, -4.8935e-01, -3.0175e-02, -1.5512e-01, -1.5891e-01,\n",
       "        -2.2415e-01,  1.7829e-01,  1.2728e-01, -1.6933e-01, -1.9553e-02,\n",
       "        -1.5901e-01, -1.0221e-01, -7.4030e-02,  1.8279e-01, -2.9809e-03,\n",
       "         4.7711e-02,  1.4232e+00,  4.5769e-02,  2.8381e-02, -8.8517e-02,\n",
       "        -2.9781e-01,  1.8585e-02, -1.5087e-01, -7.6531e-03,  5.1186e-02,\n",
       "        -1.1115e-02,  1.2207e-01,  5.3388e-02, -1.7302e-01, -1.7492e-01,\n",
       "        -9.0179e-02, -1.1674e-01, -1.6635e-01, -5.0675e-02, -1.5195e-03,\n",
       "         2.5161e-01, -1.6912e-01,  2.0180e-01, -2.5688e-01, -3.1370e-01,\n",
       "         3.9219e-03,  5.9414e-02,  3.0576e-02, -1.2894e-01, -7.0030e-02,\n",
       "         1.5464e-02, -1.1956e-01, -9.3648e-02, -8.5877e-02, -1.4017e-01,\n",
       "         2.1635e-01, -3.0639e-01,  8.6120e-02,  4.0023e-02, -2.7544e-02,\n",
       "        -3.3354e-01,  3.6070e-02,  6.2601e-01, -4.9805e-01,  7.8505e-02,\n",
       "         2.5018e-01, -1.2532e-01, -1.3538e+00,  4.2878e-02, -5.4013e-02,\n",
       "         4.5379e-02,  5.1464e-02,  1.0792e-01,  4.1086e-02,  8.1124e-02,\n",
       "         3.0966e-01,  1.0074e-01, -8.3371e-02,  1.9287e-02,  1.4378e-02,\n",
       "         1.7529e-01, -1.0093e-01, -2.7985e-02,  5.7261e-01,  1.2237e-01,\n",
       "         1.3710e-01,  1.9922e-01, -5.6349e-03,  1.8295e-02,  9.1449e-02,\n",
       "        -4.8644e-03, -1.2172e-01,  1.4629e-01,  1.2399e-02,  9.3940e-02,\n",
       "        -7.1600e-01,  5.5918e-02, -9.9526e-02, -1.4896e-01, -5.1802e-01,\n",
       "         5.3966e-02,  1.1617e-01,  1.2732e-01, -1.9289e-02, -5.6423e-02,\n",
       "        -2.1193e-01, -9.3565e-02,  9.9362e-02, -8.3388e-02,  1.7346e-01,\n",
       "         3.1380e+00,  2.4720e-01,  1.3134e-01, -8.8357e-02, -4.4635e+00,\n",
       "         1.2848e-01,  1.0695e-02,  1.4093e-02, -1.4373e-01, -1.5387e+00,\n",
       "         2.0245e-01,  2.4945e-01,  1.1976e-01,  3.2895e-02,  1.3189e-01,\n",
       "        -5.0988e-02, -1.6980e-01,  2.9799e-01, -8.1776e-02, -1.2088e-01,\n",
       "         3.4712e-02, -3.6932e-02,  1.2348e-01,  9.6342e-03,  1.2776e-01,\n",
       "         4.6924e-02,  1.2501e-01, -8.9199e-02,  2.4143e-02, -2.7593e-01,\n",
       "         2.8500e-02, -4.9482e-01, -8.6481e-03,  1.6812e-01,  6.5492e-02,\n",
       "        -9.1418e-02,  2.5600e-01,  2.1192e-02, -3.6740e-03,  1.2097e-02,\n",
       "         1.7532e-01,  5.7970e-02, -1.2311e-01, -1.3284e-01,  1.8573e-01,\n",
       "        -1.3619e-01,  2.8010e-01,  1.2182e-01, -1.0497e+00, -1.4031e-01,\n",
       "         9.0117e-01,  1.2068e-01,  3.9821e-02, -3.6422e-02,  1.3989e+00,\n",
       "         1.6301e-01,  1.1065e-01,  3.5090e-02,  5.3096e-01,  4.5175e-02,\n",
       "        -1.6227e-01,  5.9899e-03, -2.5285e-02, -9.4102e-02,  3.2319e-04,\n",
       "        -1.0006e-01, -1.3235e-01, -1.4620e-01, -1.1622e-01, -4.6344e-02,\n",
       "         9.2145e-02,  1.9295e-01, -9.5490e-03,  1.3393e-01,  1.5733e-01,\n",
       "         4.8964e-01, -3.6638e-02, -2.2840e-01, -8.7914e-02, -9.6024e-02,\n",
       "        -1.2472e-01,  3.7016e-01, -3.5890e-02,  1.1019e-01,  1.0428e-01,\n",
       "        -3.2530e-02, -1.6027e+00, -1.6566e-01, -1.1784e-02,  1.4592e-02,\n",
       "         1.8425e-01, -5.1727e-02, -2.0779e-01, -1.1194e-01, -1.0050e-02,\n",
       "         2.6103e-01,  6.7668e-02,  6.9648e-02])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(outputs.hidden_states[0]))\n",
    "print(outputs.hidden_states[0][0][0].shape) # hidden_states[generated token number][layer number] is a tensor (input length x D)\n",
    "\n",
    "hidden_states = outputs.hidden_states[0][0][0]\n",
    "\n",
    "#first token representation at end of each layer\n",
    "hidden_states[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15939d0",
   "metadata": {},
   "source": [
    "###  5. Accessing model parameters\n",
    "\n",
    "Now we retrieve the set of model parameters that are all learned during the training, and kept fixed during inference. These includes:\n",
    "\n",
    "- the embedding map, E\n",
    "- the attention matrices, in each layer and each head $(W_Q, W_K, W_V)$ \n",
    "- the neural net weights, in each layer\n",
    "\n",
    "\n",
    "When instanciating the model using \"from_pretrained()\", dropout is deactivated by default by  model.eval() (sets the model to evaluation mode). To train the model, you should first set it back in training mode with model.train()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e5b39a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3566bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "for name, weights in state_dict.items():\n",
    "    print(name, weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0017e91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = state_dict[\"transformer.wte.weight\"] # E\n",
    "lm_head_matrix = state_dict[\"lm_head.weight\"]\n",
    "torch.equal(embedding_matrix, lm_head_matrix) # True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
