{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82214333",
   "metadata": {},
   "source": [
    "# Downloading GPT2 (Generative pre-trained transformer) from Open-AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b072fc",
   "metadata": {},
   "source": [
    "### Install the transformer library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe190503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (4.51.1)\n",
      "Requirement already satisfied: filelock in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/giorgiafasiolo/opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb93918",
   "metadata": {},
   "source": [
    "### Load GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3f6d7",
   "metadata": {},
   "source": [
    "This loads the GPT-2 model and tokenizer, in the base version. \n",
    "If you want you can specify other variants like: \n",
    "\"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c2c3bc",
   "metadata": {},
   "source": [
    "- Tokenizer: Converts raw text into tokens (numbers), so the model can process it. \n",
    "\n",
    "- LMHeadModel: The GPT-2 model architecture that generates text (predicts the next token based on previous tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe6fb61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94924557",
   "metadata": {},
   "source": [
    "### Use the model (Example: Text Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f831830",
   "metadata": {},
   "source": [
    "Here we're using a pre-trained language model (GPT-2) to generate text. \n",
    "Think of this model as a super advanced autocomplete that can write full sentences or paragraphs based on an input prompt (in this case, \"Hello\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I'm sorry, but I'm not sure if you're aware of this. I'm not sure if you're aware of this. I'm not sure if you're aware of this. I'm not sure if you're aware of this. I'm not sure if you're aware of this. I'm not sure if you're aware of this. I'm not sure if you're aware of this. I'm not sure if you're aware of this. I'm not sure if\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch (needed because GPT-2 works with tensors, which are multi-dimensional arrays)\n",
    "import torch\n",
    "\n",
    "# Give a model a starting point\n",
    "input_text = \"Hello\"\n",
    "# Transformers don't read text, they read numbers!! \n",
    "# Tokenizer (here we set it up) breaks down your text into smaller pieces and maps each of them into a number (token ID)\n",
    "# return_tensors='pt' tells the tokenizer to return a PyTorch tensor (not just a list), because that's what the model expects.\n",
    "# After this line, input_ids holds a tensor like: tensor([[15496]]) — which is the ID for \"Hello\".\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Attention mask setup (for padding)\n",
    "# Tells the model which tokens it should pay attention to. 1 means \"use this token\". \n",
    "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "# Models expect a special \"padding token\" in case your input has extra space.\n",
    "# GPT-2 doesn’t have a padding token, so we use the end-of-sentence (eos) token instead.\n",
    "pad_token_id = tokenizer.eos_token_id  # Set pad token ID to eos token ID\n",
    "\n",
    "# Generate text with attention mask and pad token\n",
    "output = model.generate(input_ids, # Input in number format\n",
    "                        attention_mask=attention_mask, # Tells the model which words to pay attention to \n",
    "                        pad_token_id=pad_token_id, # Just in case the model needs padding\n",
    "                        max_length=100, # The model will stop once it has generated 100 tokens\n",
    "                        do_sample=False,# Tells the model to sample RANDOMLY instead of always choosing the most likely next word - this makes the output more creative\n",
    "                        top_k=100) # At each step, the model chooses from the top 100 most likely next words - this keeps results interesting but not too random. \n",
    "# After this line, output contains generated tokens IDs - like: tensor([[15496,  11, 318, 617, 460, 460, 318, ...]])\n",
    "\n",
    "# Convert numbers back to text\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e0fae9",
   "metadata": {},
   "source": [
    "⬆ ABOUT THE ABOVE CODE: \n",
    "Is **low-level**:\n",
    "```python\n",
    "input_ids = tokenizer.encode(...)\n",
    "output = model.generate(...)\n",
    "```\n",
    "**Characteristics:**\n",
    "- **More manual control** — You specify attention masks, padding tokens, decoding, etc.\n",
    "- You’re interacting **directly** with the model and tokenizer objects.\n",
    "- **Better for customization** — e.g., adding constraints, working with batches, doing masked generation, etc.\n",
    "- Good when you want to:\n",
    "  - Understand how generation works\n",
    "  - Build complex workflows\n",
    "  - Fine-tune models\n",
    "  - Add post-processing\n",
    " \n",
    "⬇ ABOUT THE FOLLOWING CODE: \n",
    "\n",
    "```python\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "generator(\"Hello, I'm a language model,\", ...)\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- **High-level abstraction** — It wraps the tokenizer and model together.\n",
    "- You don’t need to manually encode or decode anything.\n",
    "- **Easier and faster** for most use cases like prototyping or demos.\n",
    "- Automatically handles:\n",
    "  - Tokenization\n",
    "  - Model inference\n",
    "  - Decoding\n",
    "  - Setting padding/attention/etc. under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93899eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, and my project will get better with time, but I think there are a lot more things that can help you\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a language model, so if I don't have a problem, I can fix it by creating new words\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and I'm trying to learn some stuff. I'll try to do some basic programming and just learn better ways\"},\n",
       " {'generated_text': \"Hello, I'm a language model, but I don't believe in grammar. This will work for every language model. You can define it very quickly\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, a model of how things should be, and then we look at different things as well.\" I\\'d like to'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install tf-keras\n",
    "\n",
    "# Import pipeline: shortcut provided by Hugging Face to quickly use pre-trained models\n",
    "# Set seed of course is... to set the seed! Lol. \n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# Create a generator using PyTorch (avoid TensorFlow-related imports)\n",
    "# You pass to the generator the task tipe and the model you want to use\n",
    "# It automatically \n",
    "#   Loads the tokenizer\n",
    "#   Loads the model\n",
    "#   Decide whether to use PyTorch or TensorFlow (in this case, Pytorch)\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Actually run the generator\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bdf26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
